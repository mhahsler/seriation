[
  {
    "objectID": "seriation_methods.html",
    "href": "seriation_methods.html",
    "title": "Seriation Methods implemented in the R package seriation",
    "section": "",
    "text": "This document contains the seriation methods currently implemented in the R package seriation. The methods are organized by methods for distance matrices and data matrices.\nThis list was created for the following version of seriation:\nlibrary(\"seriation\")\npackageVersion('seriation')\n\n[1] '1.5.6'\nSome additional methods need to be registered.\nregister_DendSer()\nregister_optics()\nregister_smacof()\nregister_GA()\nregister_tsne()\nregister_umap()\nSeriation methods on the data x can be used in\nseriate(x, method = \"Spectral\", control = NULL, rep = 1L)\nThe list below shows what seriation criterion (if any) is optimized by the method.\nIf the method uses randomization (see randomized below), then rep can be used to run the algorithms several times and return the best permutation.\nAvailable control parameters can be passed on as a list using the control argument."
  },
  {
    "objectID": "seriation_methods.html#methods-for-dissimilarity-data-dist",
    "href": "seriation_methods.html#methods-for-dissimilarity-data-dist",
    "title": "Seriation Methods implemented in the R package seriation",
    "section": "Methods for dissimilarity data (dist)",
    "text": "Methods for dissimilarity data (dist)\n\nARSA\nMinimize the linear seriation criterion using simulated annealing (Brusco et al, 2008).\n\noptimizes: LS (Linear seriation criterion)\nrandomized: TRUE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ncool\n0.5\ncooling factor (smaller means faster cooling)\n\n\ntmin\n1e-04\nstopping temperature\n\n\nswap_to_inversion\n0.5\nprobability for swap vs inversion local move\n\n\ntry_multiplier\n100\nnumber of local move tries per object\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nBBURCG\nMinimize the unweighted row/column gradient by branch-and-bound (Brusco and Stahl 2005). This is only feasible for a relatively small number of objects.\n\noptimizes: Gradient_raw (Unweighted gradient condition)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\neps\n0\nDistances need to be at least eps to count as violations\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nBBWRCG\nMinimize the weighted row/column gradient by branch-and-bound (Brusco and Stahl 2005). This is only feasible for a relatively small number of objects.\n\noptimizes: Gradient_weighted (Weighted gradient condition)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\neps\n0\nDistances need to be at least eps to count as violations\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nDendSer\nDendrogram seriation (Earle and Hurley, 2015).\n\noptimizes: N/A (specified criterion restricted by dendrogram)\nrandomized: FALSE\nregistered by: register_DendSer()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nh\nNULL\nan hclust object (optional)\n\n\nmethod\n“complete”\nhclust linkage method\n\n\ncriterion\nNULL\ncriterion to optimize the dendrogram for\n\n\nDendSer_args\nNULL\nmore arguments are passed on to DendSer (? DendSer)\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nDendSer_ARc\nDendrogram seriation for Anti-Robinson form cost (Earle and Hurley, 2015).\n\noptimizes: ARc (Anti-Robinson form cost restricted by dendrogram)\nrandomized: FALSE\nregistered by: register_DendSer()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nh\nNULL\nan hclust object (optional)\n\n\nmethod\n“complete”\nhclust linkage method\n\n\ncriterion\nNULL\ncriterion to optimize the dendrogram for\n\n\nDendSer_args\nNULL\nmore arguments are passed on to DendSer (? DendSer)\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nDendSer_BAR\nDendrogram seriation with BAR (Earle and Hurley, 2015).\n\noptimizes: BAR (banded anti-Robinson form restricted by dendrogram)\nrandomized: FALSE\nregistered by: register_DendSer()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nh\nNULL\nan hclust object (optional)\n\n\nmethod\n“complete”\nhclust linkage method\n\n\ncriterion\nNULL\ncriterion to optimize the dendrogram for\n\n\nDendSer_args\nNULL\nmore arguments are passed on to DendSer (? DendSer)\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nDendSer_LPL\nDendrogram seriation for Lazy path length (Earle and Hurley, 2015).\n\noptimizes: Lazy_path_length (restricted by dendrogram)\nrandomized: FALSE\nregistered by: register_DendSer()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nh\nNULL\nan hclust object (optional)\n\n\nmethod\n“complete”\nhclust linkage method\n\n\ncriterion\nNULL\ncriterion to optimize the dendrogram for\n\n\nDendSer_args\nNULL\nmore arguments are passed on to DendSer (? DendSer)\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nDendSer_PL\nDendrogram seriation for Path length (Earle and Hurley, 2015).\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\nregistered by: register_DendSer()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nh\nNULL\nan hclust object (optional)\n\n\nmethod\n“complete”\nhclust linkage method\n\n\ncriterion\nNULL\ncriterion to optimize the dendrogram for\n\n\nDendSer_args\nNULL\nmore arguments are passed on to DendSer (? DendSer)\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nEnumerate\nEnumerate all permutations\n\noptimizes: N/A (set via control criterion))\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ncriterion\n“Gradient_weighted”\nCriterion measure to optimize\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nGA\nUse a genetic algorithm to optimize for various criteria.\n\noptimizes: N/A (specified as parameter criterion)\nrandomized: TRUE\nregistered by: register_GA()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ncriterion\n“BAR”\ncriterion to be optimized\n\n\nsuggestions\nc(“TSP”, “QAP_LS”, “Spectral”)\nseed the population with these seriation methods\n\n\nselection\nfunction (object, r = 2/(object@popSize * (object@popSize - 1)), q = 2/object@popSize, …) {\nselection operator function\n\n\ncrossover\nfunction (object, parents, …) { if (gaControl(“useRcpp”)) gaperm_oxCrossover_Rcpp(objec\ncrossover operator function\n\n\nmutation\nfunction (object, parent, …) { if (runif(1) &gt; ismProb) GA::gaperm_simMutation(object, p\nmutation operator function\n\n\npcrossover\n0.8\nprobability for crossover\n\n\npmutation\n0.1\nptobability of mutations\n\n\npopSize\n100\npopulation size\n\n\nmaxiter\n1000\nmaximum number of generations\n\n\nrun\n50\nstop after run generations without improvement\n\n\nparallel\nFALSE\nuse multiple cores?\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nGSA\nMinimize a specified seriation measure (criterion) using simulated annealing.\n\noptimizes: N/A (set via control criterion)\nrandomized: TRUE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ncriterion\n“Gradient_raw”\nCriterion measure to optimize\n\n\ncool\n0.5\ncooling factor (smaller means faster cooling)\n\n\nt_min\n1e-07\nstopping temperature\n\n\nlocalsearch\n“LS_insert”\nused local search move function\n\n\ntry_multiplier\n5\nnumber of local move tries per object\n\n\nt0\nNA\ninitial temperature (if NA then it is estimated)\n\n\np_initial_accept\n0.01\nProbability to accept a bad move at time 0 (used for t0 estimation)\n\n\nwarmstart\n“Random”\npermutation or seriation method for warmstart\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nGW\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by the Gruvaeus and Wainer (1972) heuristic\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nhclust\nNULL\na precomputed hclust object (optional)\n\n\nlinkage\n“complete”\nhclust method\n\n\n\n\n\nGW_average\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by the Gruvaeus and Wainer (1972) heuristic (avg.link)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nGW_complete\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by the Gruvaeus and Wainer (1972) heuristic (complete link)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nGW_single\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by the Gruvaeus and Wainer (1972) heuristic (single link)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nGW_ward\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by the Gruvaeus and Wainer (1972) heuristic (Ward’s method)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nHC\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nhclust\nNULL\na precomputed hclust object (optional)\n\n\nlinkage\n“complete”\nhclust method\n\n\n\n\n\nHC_average\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering (avg. link).\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nHC_complete\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering (complete link).\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nHC_single\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering (single link)\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nHC_ward\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering (Ward’s method).\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nIdentity\nIdentity permutation\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nisomap\nIsometric feature mapping ordination\n\noptimizes: N/A (Stress on shortest path distances)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nk\n30\nnumber of shortest dissimilarities retained for a point\n\n\npath\n“shortest”\nmethod used in to estimate the shortest path (“shortest”/“extended”)\n\n\n\n\n\nisoMDS\nOrder along the 1D Kruskal’s non-metric multidimensional scaling\n\noptimizes: MDS_stress (with monotonic transformation)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nadd\n1e-09\nsmall constant to avoid 0 distances\n\n\nmaxit\n50\nmaximum number of iterations\n\n\ntrace\nFALSE\ntrace optimization\n\n\ntol\n0.001\nconvergence tolerance\n\n\np\n2\npower for Minkowski distance in the configuration space\n\n\n\n\n\nMDS\nOrder along the 1D classical metric multidimensional scaling\n\noptimizes: MDS_stress (Euclidean distances)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nadd\nFALSE\nmake the distances Euclidean using an additive constant (see ? cmdscale)\n\n\n\n\n\nMDS_angle\nOrder by the angular order in the 2D MDS projection space split by the larges gap\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nadd\nFALSE\nmake the distances Euclidean using an additive constant (see ? cmdscale)\n\n\n\n\n\nMDS_smacof\nSeriation based on multidemensional scaling using stress majorization (de Leeuw & Mair, 2009).\n\noptimizes: smacof_stress0 (MDS stress)\nrandomized: FALSE\nregistered by: register_smacof()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ntype\n“ratio”\nMDS type: “interval”, “ratio”, “ordinal” (nonmetric MDS)\n\n\ninit\n“torgerson”\nstart configuration method (“torgerson”/“random”)\n\n\nrelax\nFALSE\nuse block relaxation for majorization?\n\n\nmodulus\n1\nnumber of smacof iterations per monotone regression call\n\n\nitmax\n1000\nmaximum number of iterations\n\n\neps\n1e-06\nconvergence criterion\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nmetaMDS\nNonmetric Multidimensional Scaling with Stable Solution from Random Starts.\n\noptimizes: MDS_stress (Kruskal’s monotone regression stress)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ndistance\n“bray”\nsee ? metaMDS for help\n\n\ntry\n20\nN/A\n\n\ntrymax\n20\nN/A\n\n\nengine\n“monoMDS”\nN/A\n\n\nautotransform\nTRUE\nN/A\n\n\nnoshare\nFALSE\nN/A\n\n\nwascores\nTRUE\nN/A\n\n\nexpand\nTRUE\nN/A\n\n\ntrace\n0\nN/A\n\n\nplot\nFALSE\nN/A\n\n\nprevious.best\n\nN/A\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nmonoMDS\nKruskal’s (1964a,b) non-metric multidimensional scaling (NMDS) using monotone regression.\n\noptimizes: MDS_stress (Kruskal’s monotone regression stress)\nrandomized: TRUE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ny\n\nSee ? monoMDS for help\n\n\nmodel\n“global”\nN/A\n\n\nthreshold\n0.8\nN/A\n\n\nmaxit\n200\nN/A\n\n\nweakties\nTRUE\nN/A\n\n\nstress\n1\nN/A\n\n\nscaling\nTRUE\nN/A\n\n\npc\nTRUE\nN/A\n\n\nsmin\n1e-04\nN/A\n\n\nsfgrmin\n1e-07\nN/A\n\n\nsratmax\n0.999999\nN/A\n\n\n\n\n\nOLO\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by with optimal leaf ordering (Bar-Joseph et al., 2001)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nhclust\nNULL\na precomputed hclust object (optional)\n\n\nlinkage\n“complete”\nhclust method\n\n\n\n\n\nOLO_average\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by with optimal leaf ordering (Bar-Joseph et al., 2001) (avg. link)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nOLO_complete\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by with optimal leaf ordering (Bar-Joseph et al., 2001) (complete link)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nOLO_single\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by with optimal leaf ordering (Bar-Joseph et al., 2001) (single link)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nOLO_ward\nUsing the order of the leaf nodes in a dendrogram obtained by hierarchical clustering and reordered by with optimal leaf ordering (Bar-Joseph et al., 2001) (Ward’s method)\n\noptimizes: Path_length (restricted by dendrogram)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\noptics\nUse ordering points to identify the clustering structure (OPTICS) to create an order\n\noptimizes: N/A\nrandomized: FALSE\nregistered by: register_optics()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\neps\nNULL\nupper limit of the size of the epsilon neighborhood (see ? optics)\n\n\nminPts\n5\nminimum density for dense neighborhoods\n\n\n\n\n\nQAP_2SUM\nQuadratic assignment problem formulation for seriation solved using a simulated annealing solver to minimize the 2-Sum Problem criterion (Barnard, Pothen, and Simon 1993).\n\noptimizes: 2SUM (2-sum criterion)\nrandomized: TRUE\ncontrol parameters: no parameters\n\n\n\nQAP_BAR\nQuadratic assignment problem formulation for seriation solved using a simulated annealing solver to minimize the banded anti-Robinson form (Hahsler, 2017).\n\noptimizes: BAR (Banded anti-robinson form)\nrandomized: TRUE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nb\nfunction (n) max(1, floor(n * 0.2))\nbandwidth (default is 20%)\n\n\n\n\n\nQAP_Inertia\nQuadratic assignment problem formulation for seriation solved using a simulated annealing solver to minimize the Inertia criterion (Hahsler, 2017).\n\noptimizes: Inertia\nrandomized: TRUE\ncontrol parameters: no parameters\n\n\n\nQAP_LS\nQuadratic assignment problem formulation for seriation solved using a simulated annealing solver to minimize the Linear Seriation Problem (LS) criterion (Hubert and Schultz 1976).\n\noptimizes: LS (Linear seriation criterion)\nrandomized: TRUE\ncontrol parameters: no parameters\n\n\n\nR2E\nRank-two ellipse seriation (Chen 2002)\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nRandom\nRandom permutation\n\noptimizes: N/A\nrandomized: TRUE\ncontrol parameters: no parameters\n\n\n\nReverse\nReversed identity permutation\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nSammon_mapping\nOrder along the 1D Sammon’s non-linear mapping\n\noptimizes: MDS_stress (scale free, weighted stress called Sammon’s error)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nadd\n1e-09\nsmall constant to avoid 0 distances\n\n\nniter\n100\nmaximum number of iterations\n\n\ntrace\nFALSE\ntrace optimization\n\n\nmagic\n0.2\ninitial value of the step size constant in diagonal Newton method\n\n\ntol\n1e-04\ntolerance for stopping in units of stress\n\n\n\n\n\nSGD\nImprove an existing solution using stochastic gradient descent.\n\noptimizes: N/A (set via control criterion)\nrandomized: TRUE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ncriterion\n“Gradient_raw”\nCriterion measure to optimize\n\n\ninit\n“Spectral”\nStart permutation or name of a seriation method\n\n\nmax_iter\nNULL\nnumber of iterations\n\n\nlocalsearch\n“LS_insert”\nused local search move function\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nSpectral\nSpectral seriation (Ding and He 2004) uses a relaxation to minimize the 2-Sum Problem (Barnard, Pothen, and Simon 1993). It uses the order of the Fiedler vector of the similarity matrix’s Laplacian.\n\noptimizes: 2SUM (2-sum criterion)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nSpectral_norm\nSpectral seriation (Ding and He 2004) uses a relaxation to minimize the 2-Sum Problem (Barnard, Pothen, and Simon 1993). It uses the order of the Fiedler vector of the similarity matrix’s normalized Laplacian.\n\noptimizes: 2SUM (2-sum criterion)\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nSPIN_NH\nSorting Points Into Neighborhoods (SPIN) (Tsafrir 2005). Neighborhood algorithm to concentrate low distance values around the diagonal.\n\noptimizes: N/A (Energy)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nsigma\nc(20, 17, 15, 13, 11, 9, 7, 5, 3, 1)\nemphasize local (small alpha) or global (large alpha) structure.\n\n\nstep\n5\niterations to run for each sigma value.\n\n\nW_function\nNULL\ncustom function to create the weight matrix W\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nSPIN_STS\nSorting Points Into Neighborhoods (SPIN) (Tsafrir 2005). Side-to-Side algorithm which tries to push out large distance values.\n\noptimizes: N/A (Energy)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nstep\n25L\niterations to run\n\n\nnstart\n10L\nnumber of random restarts\n\n\nX\nfunction (n) seq(n) - (n + 1)/2\nmatrix to calculate the W matrix\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\ntsne\nUse 1D t-distributed stochastic neighbor embedding (t-SNE) a distance matrix to create an order (van der Maaten and Hinton, 2008).\n\noptimizes: N/A\nrandomized: TRUE\nregistered by: register_tsne()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nmax_iter\n1000\nnumber of iterations\n\n\ntheta\n0.5\nspeed/accuracy trade-off (increase for less accuracy)\n\n\nperplexity\nNULL\nperplexity parameter (calculated as n - 1 / 3)\n\n\neta\n100\nlearning rate\n\n\nmds\nTRUE\nstart from a classical MDS solution\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nTSP\nMinimize Hamiltonian path length with a TSP solver.\n\noptimizes: Path_length\nrandomized: TRUE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nmethod\n“arbitrary insertion”\nused TSP method (see ? solve_TSP)\n\n\nrep\n10\nnumber of random restarts\n\n\ntwo_opt\nTRUE\nuse the 2-opt improvement heuristic?\n\n\n\n\n\numap\nUse 1D Uniform manifold approximation and projection (UMAP) embedding of the distances to create an order (McInnes and Healy, 2018)\n\noptimizes: N/A\nrandomized: TRUE\nregistered by: register_umap()\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nn_neighbors\nNA\nsee ? umap::umap for help\n\n\nn_components\n1\nN/A\n\n\nmetric\n“euclidean”\nN/A\n\n\nn_epochs\n1000\nN/A\n\n\ninput\nNA\nN/A\n\n\ninit\n“spectral”\nN/A\n\n\nmin_dist\n0.1\nN/A\n\n\nset_op_mix_ratio\n1\nN/A\n\n\nlocal_connectivity\n1\nN/A\n\n\nbandwidth\n1\nN/A\n\n\nalpha\n0.001\nN/A\n\n\ngamma\n1\nN/A\n\n\nnegative_sample_rate\n5\nN/A\n\n\na\nNA\nN/A\n\n\nb\nNA\nN/A\n\n\nspread\n1\nN/A\n\n\nrandom_state\nNA\nN/A\n\n\ntransform_state\nNA\nN/A\n\n\nknn\nNA\nN/A\n\n\nknn_repeats\n1\nN/A\n\n\nverbose\nFALSE\nN/A\n\n\numap_learn_args\nNA\nN/A\n\n\n\n\n\nVAT\nVisual assesment of clustering tendency (Bezdek and Hathaway (2002). Creates an order based on Prim’s algorithm for finding a minimum spanning tree (MST) in a weighted connected graph representing the distance matrix. The order is given by the order in which the nodes (objects) are added to the MST.\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters"
  },
  {
    "objectID": "seriation_methods.html#methods-for-data-matrices-tables-and-data.frames",
    "href": "seriation_methods.html#methods-for-data-matrices-tables-and-data.frames",
    "title": "Seriation Methods implemented in the R package seriation",
    "section": "Methods for data matrices, tables and data.frames",
    "text": "Methods for data matrices, tables and data.frames\n\nAOE\nOrder by the angle of the first two eigenvectors for correlation matrices (Friendly, 2002)\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nBEA\nBond Energy Algorithm (BEA; McCormick 1972) to maximize the Measure of Effectiveness of a non-negative matrix.\n\noptimizes: ME (Measure of effectiveness)\nrandomized: TRUE\ncontrol parameters: no parameters\n\n\n\nBEA_TSP\nUse a TSP to optimize the Measure of Effectiveness (Lenstra 1974).\n\noptimizes: ME (Measure of effectiveness)\nrandomized: TRUE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nmethod\n“arbitrary insertion”\nused TSP method (see ? solve_TSP)\n\n\nrep\n10\nnumber of random restarts\n\n\ntwo_opt\nTRUE\nuse the 2-opt improvement heuristic?\n\n\n\n\n\nBK_unconstrained\nImplements the method for binary matrices described in Brower and Kile (1988). Reorders using the mean row and column position of presences (1s).\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nCA\nThis method calculates a correspondence analysis of the matrix and computes an order according to the scores on a correspondence analysis dimension (Friendly 2023).\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ndim\n1L\nCA dimension used for reordering\n\n\nca_param\nNULL\nList with parameters for the call to ca::ca()\n\n\n\n\n\nHeatmap\nCalculates distances for rows and columns and then independently applies the specified seriation method for distances.\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ndist_fun\nlist(row = function (x, method = “euclidean”, diag = FALSE, upper = FALSE, p = 2) { if (!is.n\nA named list with functions to calulate row and column distances\n\n\nseriation_method\nlist(row = “OLO_complete”, col = “OLO_complete”)\nA named list with row and column seriation methods\n\n\nseriation_control\nlist(row = NULL, col = NULL)\nnamed list with control parameters for the seriation methods\n\n\nscale\n“none”\nScale “rows”, “cols”, or “none”\n\n\nverbose\nFALSE\nN/A\n\n\n\n\n\nIdentity\nIdentity permutation\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\nLLE\nFind an order using 1D locally linear embedding.\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nk\n30\nused number of neighbors\n\n\nreg\n2\nregularization method (see ? lle)\n\n\n\n\n\nMean\nReorders rows and columns by row and column means.\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ntransformation\nNULL\ntransformation function applied before calculating means (e.g., scale)\n\n\n\n\n\nPCA\nUses the projection of the data on its first principal component to determine the order.\n\noptimizes: N/A (Least squares for each dimension (for Euclidean distances).)\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ncenter\nTRUE\ncenter the data (mean = 0)?\n\n\nscale\nFALSE\nscale to unit variance?\n\n\nverbose\nFALSE\nFALSE\n\n\n\n\n\nPCA_angle\nUses the angular order in the 2D PCA projection space split by the larges gap.\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ncenter\nTRUE\ncenter the data (mean = 0)?\n\n\nscale\nFALSE\nscale to unit variance?\n\n\nverbose\nFALSE\nFALSE\n\n\n\n\n\nRandom\nRandom permutation\n\noptimizes: N/A\nrandomized: TRUE\ncontrol parameters: no parameters\n\n\n\nReverse\nReversed identity permutation\n\noptimizes: N/A\nrandomized: FALSE\ncontrol parameters: no parameters\n\n\n\ntsne\nUse 1D t-distributed stochastic neighbor embedding (t-SNE) of the rows of a matrix to create an order (van der Maaten and Hinton, 2008).\n\noptimizes: N/A\nrandomized: TRUE\nregistered by: register_tsne()\ncontrol parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nmax_iter\n1000\nnumber of iterations\n\n\ntheta\n0.5\nspeed/accuracy trade-off (increase for less accuracy)\n\n\nperplexity\nNULL\nperplexity parameter (calculated as n - 1 / 3)\n\n\neta\n100\nlearning rate\n\n\npca\nTRUE\nstart the PCA solution\n\n\n\n\n\numap\nUse 1D Uniform manifold approximation and projection (UMAP) embedding of the data to create an order (McInnes and Healy, 2018)\n\noptimizes: N/A\nrandomized: TRUE\nregistered by: register_umap()\ncontrol parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nn_neighbors\nNA\nsee ? umap::umap for help\n\n\nn_components\n1\nN/A\n\n\nmetric\n“euclidean”\nN/A\n\n\nn_epochs\n1000\nN/A\n\n\ninput\nNA\nN/A\n\n\ninit\n“spectral”\nN/A\n\n\nmin_dist\n0.1\nN/A\n\n\nset_op_mix_ratio\n1\nN/A\n\n\nlocal_connectivity\n1\nN/A\n\n\nbandwidth\n1\nN/A\n\n\nalpha\n0.001\nN/A\n\n\ngamma\n1\nN/A\n\n\nnegative_sample_rate\n5\nN/A\n\n\na\nNA\nN/A\n\n\nb\nNA\nN/A\n\n\nspread\n1\nN/A\n\n\nrandom_state\nNA\nN/A\n\n\ntransform_state\nNA\nN/A\n\n\nknn\nNA\nN/A\n\n\nknn_repeats\n1\nN/A\n\n\nverbose\nFALSE\nN/A\n\n\numap_learn_args\nNA\nN/A"
  },
  {
    "objectID": "seriation_criteria.html",
    "href": "seriation_criteria.html",
    "title": "Seriation Criteria implemented in the R package seriation",
    "section": "",
    "text": "This document contains the seriation criteria for judging the quality of permutations given data implemented in the R package seriation.\nThis list was created for the following version of seriation:\nlibrary(\"seriation\")\npackageVersion('seriation')\n\n[1] '1.5.5.1'\nRegister some additional criteria\nregister_DendSer()\nregister_smacof()\nThe criteria are organized by methods that evaluate the permutation based on distance data or data matrices.\nmerit specified if the criteria function increases with better fit or if it is formulated as a loss criteria functions (merit = FALSE).\nIf a seriation method directly tries to optimize the criterion, than its name is specified under “optimized by”. The names can be used as the method argument in seriate()."
  },
  {
    "objectID": "seriation_criteria.html#criteria-for-dissimilarity-data-dist",
    "href": "seriation_criteria.html#criteria-for-dissimilarity-data-dist",
    "title": "Seriation Criteria implemented in the R package seriation",
    "section": "Criteria for dissimilarity data (dist)",
    "text": "Criteria for dissimilarity data (dist)\n\n2SUM\n2-Sum Criterion: The 2-Sum loss criterion multiplies the similarity between objects with the squared rank differences (Barnard, Pothen and Simon, 1993).\n\nmerit: FALSE\noptimized by: QAP_2SUM, Spectral, Spectral_norm\nadditional parameters: no parameters\n\n\n\nAR_deviations\nAnti-Robinson deviations: The number of violations of the anti-Robinson form weighted by the deviation (Chen, 2002).\n\nmerit: FALSE\noptimized by: N/A\nadditional parameters: no parameters\n\n\n\nAR_events\nAnti-Robinson events: The number of violations of the anti-Robinson form (Chen, 2002).\n\nmerit: FALSE\noptimized by: N/A\nadditional parameters: no parameters\n\n\n\nARc\nAnti-Robinson form cost (Earle and Hurley, 2015).\n\nmerit: FALSE\noptimized by: DendSer_ARc\nregistered by: register_DendSer()\nadditional parameters: no parameters\n\n\n\nBAR\nBanded Anti-Robinson form criterion: Measure for closeness to the anti-Robinson form in a band of size b (Earle and Hurley, 2015).\n\nmerit: FALSE\noptimized by: QAP_BAR, DendSer_BAR\nadditional parameters:\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nb\nNULL\nband size defaults to a band of 20% of n\n\n\n\n\n\nGradient_raw\nGradient measure: Evaluates how well distances increase when moving away from the diagonal of the distance matrix (Hubert et al, 2001).\n\nmerit: TRUE\noptimized by: BBURCG\nadditional parameters: no parameters\n\n\n\nGradient_weighted\nGradient measure (weighted): Evaluates how well distances increase when moving away from the diagonal of the distance matrix (Hubert et al, 2001).\n\nmerit: TRUE\noptimized by: BBWRCG\nadditional parameters: no parameters\n\n\n\nInertia\nInertia criterion: Measures the moment of the inertia of dissimilarity values around the diagonal of the distance matrix (Caraux and Pinloche, 2005).\n\nmerit: TRUE\noptimized by: QAP_Inertia\nadditional parameters: no parameters\n\n\n\nLazy_path_length\nLazy path length: A weighted version of the Hamiltonian path criterion where later distances are less important (Earl and Hurley, 2015).\n\nmerit: FALSE\noptimized by: DendSer_LPL\nadditional parameters: no parameters\n\n\n\nLeast_squares\nLeast squares criterion: The sum of squared differences between distances and the rank differences (Caraux and Pinloche, 2005).\n\nmerit: FALSE\noptimized by: N/A\nadditional parameters: no parameters\n\n\n\nLS\nLinear Seriation Criterion: Weights the distances with the absolute rank differences (Hubert and Schultz, 1976).\n\nmerit: FALSE\noptimized by: ARSA, QAP_LS\nadditional parameters: no parameters\n\n\n\nMDS_stress\nNormalized stress of a configuration given by a seriation order\n\nmerit: FALSE\noptimized by: MDS, isoMDS, Sammon_mapping, monoMDS, metaMDS\nadditional parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nwarn\nFALSE\nproduce a warning if the 1D MDS fit does not preserve the given order (see ? seriation::uniscale).\n\n\n\n\n\nME\nMeasure of effectiveness applied to the reordered similarity matrix (McCormick, 1972).\n\nmerit: TRUE\noptimized by: BEA, BEA_TSP\nadditional parameters: no parameters\n\n\n\nMoore_stress\nStress criterion (Moore neighborhood) applied to the reordered similarity matrix (Niermann, 2005).\n\nmerit: FALSE\noptimized by: N/A\nadditional parameters: no parameters\n\n\n\nNeumann_stress\nStress criterion (Neumann neighborhood) applied to the reordered similarity matrix (Niermann, 2005).\n\nmerit: FALSE\noptimized by: N/A\nadditional parameters: no parameters\n\n\n\nPath_length\nHamiltonian path length: Sum of distances by following the permutation (Caraux and Pinloche, 2005).\n\nmerit: FALSE\noptimized by: TSP, GW, GW_single, GW_average, GW_complete, GW_ward, OLO, OLO_single, OLO_average, OLO_complete, OLO_ward, DendSer_PL\nadditional parameters: no parameters\n\n\n\nRGAR\nRelative generalized anti-Robinson events: Counts Anti-Robinson events in a variable band of size w around the main diagonal and normalizes by the maximum of possible events (Tien et al, 2008).\n\nmerit: FALSE\noptimized by: N/A\nadditional parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\nw\nNULL\nwindow size. Default is to use a pct of 100% of n\n\n\npct\n100\nspecify w as a percentage of n in (0,100]\n\n\nrelative\nTRUE\nset to FALSE to get the GAR, i.e., the absolute number of AR events in the window.\n\n\n\n\n\nRho\nAbsolute value of the Spearman rank correlation between original distances and rank differences of the order.\n\nmerit: TRUE\noptimized by: N/A\nadditional parameters: no parameters\n\n\n\nsmacof_stress0\nStress0 calculated for different transformation types from package smacof.\n\nmerit: FALSE\noptimized by: MDS_smacof\nregistered by: register_smacof()\nadditional parameters:\n\n\n\n\n\n\n\n\n\n\ndefault\nhelp\n\n\n\n\ntype\n“ratio”\nMDS type (see ? smacof::stress0)\n\n\nwarn\nFALSE\nproduce a warning if the 1D MDS fit does not preserve the given order (see ? seriation::uniscale).\n\n\nmore\nNA\nmore arguments are passed on to smacof::stress0."
  },
  {
    "objectID": "seriation_criteria.html#criteria-for-data-matrices-tables-and-data.frames",
    "href": "seriation_criteria.html#criteria-for-data-matrices-tables-and-data.frames",
    "title": "Seriation Criteria implemented in the R package seriation",
    "section": "Criteria for data matrices, tables and data.frames",
    "text": "Criteria for data matrices, tables and data.frames\n\nCor_R\nWeighted correlation coefficient R: A measure of effectiveness normalized between -1 and 1 (Deutsch and Martin, 1971).\n\nmerit: TRUE\noptimized by: N/A\nadditional parameters: no parameters\n\n\n\nME\nMeasure of effectiveness (McCormick, 1972).\n\nmerit: TRUE\noptimized by: BEA, BEA_TSP\nadditional parameters: no parameters\n\n\n\nMoore_stress\nStress criterion (Moore neighborhood) applied to the reordered matrix (Niermann, 2005).\n\nmerit: FALSE\noptimized by: N/A\nadditional parameters: no parameters\n\n\n\nNeumann_stress\nStress criterion (Neumann neighborhood) applied to the reordered matrix (Niermann, 2005).\n\nmerit: FALSE\noptimized by: N/A\nadditional parameters: no parameters"
  },
  {
    "objectID": "heatmaps.html",
    "href": "heatmaps.html",
    "title": "Heatmaps with Package Seriation",
    "section": "",
    "text": "A Heatmap uses colored tiles to represent the values in a data matrix. Patterns can be easily seen if the rows and columns are appropriately reordered. There are many ways to reorder a matrix and the order has a huge impact on how useful the visualization is. The package seriation implements a large number of reordering methods (see: the list with all implemented seriation methods). seriation also provides a set of functions to display reordered heatmaps:\n\npimage()\nhmap()\ngghmap()\n\nHow to cite the seriation package:\n\nHahsler M, Hornik K, Buchta C (2008). “Getting things in order: An introduction to the R package seriation.” Journal of Statistical Software, 25(3), 1-34. ISSN 1548-7660, doi:10.18637/jss.v025.i03 https://doi.org/10.18637/jss.v025.i03.\n\n\n\nAs an example, we use the Wood dataset with the normalized gene expression data (a sample of 136 genes) for wood formation in poplar trees in 6 locations. In case the data has alread some order, we randomly reorder rows and columns for this example.\n\nif (!require(\"seriation\")) install.packages(\"seriation\")\n\nLoading required package: seriation\n\nlibrary(\"seriation\")\ndata(\"Wood\")\nWood &lt;- Wood[sample(nrow(Wood)), sample(ncol(Wood))]\ndim(Wood)\n\n[1] 136   6\n\nDT::datatable(round(Wood, 2))\n\n\n\n\n\n\nHere is a simple heatmap without reordering. No structure is visible.\n\npimage(Wood)\n\n\n\n\n\n\n\n\n\n\n\nMany seriation methods are available. The manual page for seriate() describes the methods available in package seriation.\nMethods of interest for heatmaps are dendrogram leaf order-based methods applied to rows and columns. This is done using method = \"heatmap\". The actual seriation method can be passed on as parameter seriaton_method, but it has a suitable default if it is omitted. Here is an example:\n\no &lt;- seriate(Wood, method = \"Heatmap\", seriation_method = \"HC_Mean\")\no\n\nobject of class 'ser_permutation', 'list'\ncontains permutation vectors for 2-mode data\n\n  vector length seriation method\n1           136          HC_Mean\n2             6          HC_Mean\n\n\nThis is the order for rows and columns. The method heatmap automatically performs hierarchical clustering and then applies the seriation method for reordering of dendrogram leafs. Here we use the row/column mean to reorder the dendrogram. The resulting order (2 means second dimension, i.e., columns) can be shown, and the reordered dendrogram and a reorderd image can be plotted.\n\nget_order(o, 2)\n\nB A P E C D \n3 6 1 5 2 4 \n\n\n\nplot(o[[2]])\npimage(Wood, order = o)"
  },
  {
    "objectID": "heatmaps.html#prepare-the-data",
    "href": "heatmaps.html#prepare-the-data",
    "title": "Heatmaps with Package Seriation",
    "section": "",
    "text": "As an example, we use the Wood dataset with the normalized gene expression data (a sample of 136 genes) for wood formation in poplar trees in 6 locations. In case the data has alread some order, we randomly reorder rows and columns for this example.\n\nif (!require(\"seriation\")) install.packages(\"seriation\")\n\nLoading required package: seriation\n\nlibrary(\"seriation\")\ndata(\"Wood\")\nWood &lt;- Wood[sample(nrow(Wood)), sample(ncol(Wood))]\ndim(Wood)\n\n[1] 136   6\n\nDT::datatable(round(Wood, 2))\n\n\n\n\n\n\nHere is a simple heatmap without reordering. No structure is visible.\n\npimage(Wood)"
  },
  {
    "objectID": "heatmaps.html#reordering-in-seriation",
    "href": "heatmaps.html#reordering-in-seriation",
    "title": "Heatmaps with Package Seriation",
    "section": "",
    "text": "Many seriation methods are available. The manual page for seriate() describes the methods available in package seriation.\nMethods of interest for heatmaps are dendrogram leaf order-based methods applied to rows and columns. This is done using method = \"heatmap\". The actual seriation method can be passed on as parameter seriaton_method, but it has a suitable default if it is omitted. Here is an example:\n\no &lt;- seriate(Wood, method = \"Heatmap\", seriation_method = \"HC_Mean\")\no\n\nobject of class 'ser_permutation', 'list'\ncontains permutation vectors for 2-mode data\n\n  vector length seriation method\n1           136          HC_Mean\n2             6          HC_Mean\n\n\nThis is the order for rows and columns. The method heatmap automatically performs hierarchical clustering and then applies the seriation method for reordering of dendrogram leafs. Here we use the row/column mean to reorder the dendrogram. The resulting order (2 means second dimension, i.e., columns) can be shown, and the reordered dendrogram and a reorderd image can be plotted.\n\nget_order(o, 2)\n\nB A P E C D \n3 6 1 5 2 4 \n\n\n\nplot(o[[2]])\npimage(Wood, order = o)"
  },
  {
    "objectID": "heatmaps.html#without-dendrograms-pimage",
    "href": "heatmaps.html#without-dendrograms-pimage",
    "title": "Heatmaps with Package Seriation",
    "section": "Without dendrograms: pimage",
    "text": "Without dendrograms: pimage\nThe permutation image plot in seriation provides a simple heatmap. The order argument not only accepts a seriation order, but also a seriation method.\n\npimage(Wood, order = \"Heatmap\", seriation_method = \"HC_complete\", \n       main = \"Wood (hierarichal clustering)\")\npimage(Wood, order = \"Heatmap\", seriation_method = \"HC_Mean\", \n       main = \"Wood (reorder by row/col mean)\")\npimage(Wood, order = \"Heatmap\", seriation_method = \"GW_complete\", \n       main = \"Wood (reorder by Gruvaeus and Wainer heuristic)\")\n\nRegistered S3 method overwritten by 'gclus':\n  method         from     \n  reorder.hclust seriation\n\npimage(Wood, order = \"Heatmap\", \n       main = \"Wood (default - optimal leaf ordering)\")"
  },
  {
    "objectID": "heatmaps.html#with-dendrograms-hmap",
    "href": "heatmaps.html#with-dendrograms-hmap",
    "title": "Heatmaps with Package Seriation",
    "section": "With dendrograms: hmap",
    "text": "With dendrograms: hmap\nHere are some typical reordering schemes.\n\nhmap(Wood, method = \"HC_complete\", main = \"Wood (hierarichal clustering)\")\nhmap(Wood, method = \"HC_Mean\", main = \"Wood (reorder by row/col mean)\")\nhmap(Wood, method = \"GW_complete\", main = \"Wood (reorder by Gruvaeus and Wainer heuristic)\")\nhmap(Wood, method = \"OLO_complete\", main = \"Wood (opt. leaf ordering)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent linkage types can be added in the method name.\nPackage DendSer offers more dendrogram seriation methods. These methods can be registered using `register_DendSer()``\n\nregister_DendSer()\n\nRegistering new seriation method 'DendSer' for 'dist'register_DendSer()\n\n\nRegistering new seriation method 'DendSer_BAR' for 'dist'register_DendSer()\n\n\nRegistering new seriation method 'DendSer_PL' for 'dist'register_DendSer()\n\n\nRegistering new seriation method 'DendSer_LPL' for 'dist'register_DendSer()\n\n\nRegistering new seriation method 'DendSer_ARc' for 'dist'register_DendSer()\n\n\nRegistering new seriation criteron 'ARc' for 'dist' using register_DendSer()\n\nhmap(Wood, method = \"DendSer_BAR\", main = \"Wood (banded anti-Robinson)\")"
  },
  {
    "objectID": "heatmaps.html#with-distance-matrices-instead-of-dendrograms-hmap",
    "href": "heatmaps.html#with-distance-matrices-instead-of-dendrograms-hmap",
    "title": "Heatmaps with Package Seriation",
    "section": "With distance matrices instead of dendrograms: hmap",
    "text": "With distance matrices instead of dendrograms: hmap\nInstead of dendrograms, also reordered distance matrices can be displayed. Dark block around the diagonal indicate the cluster structure.\n\nhmap(Wood, method = \"HC_complete\", \n     plot_margins = \"distances\",\n     main = \"Wood (hierarichal clustering)\")\n\n\n\n\n\n\n\n\nAlso non-dendrogram-based reordering methods can be used. These methods reorder rows and columns. Instead of the dendrograms, reordered distance matrices are shown.\n\nhmap(Wood, method = \"MDS\", main = \"Wood (MDS)\")\nhmap(Wood, method = \"MDS_angle\", main = \"Wood (Angle in 2D MDS space)\")\nhmap(Wood, method = \"R2E\", main = \"Wood (Rank 2 ellipse seriation)\")\nhmap(Wood, method = \"TSP\", main = \"Wood (Traveling salesperson)\")"
  },
  {
    "objectID": "heatmaps.html#colors-with-pimage-and-hmap",
    "href": "heatmaps.html#colors-with-pimage-and-hmap",
    "title": "Heatmaps with Package Seriation",
    "section": "colors with pimage and hmap",
    "text": "colors with pimage and hmap\n\nhmap(Wood, col = grays())\nhmap(Wood, col = greenred())\n\nhmap(Wood, col = colorRampPalette(c(\"brown\", \"orange\", \"red\"))( 100 ) )\n\nif (!require(\"viridis\")) install.packages(\"viridis\")\n\nLoading required package: viridis\n\n\nLoading required package: viridisLite\n\nhmap(Wood, col = viridis::viridis(100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many other packages to create color palettes in R like RColorBrewer or colorspaces."
  },
  {
    "objectID": "heatmaps.html#ggplot2",
    "href": "heatmaps.html#ggplot2",
    "title": "Heatmaps with Package Seriation",
    "section": "ggplot2",
    "text": "ggplot2\nAll options are also available for ggplot2 using gghmap(). Currently there is no support to display dendrograms.\n\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\ngghmap(Wood, method = \"OLO\")"
  },
  {
    "objectID": "heatmaps.html#heatmap-in-package-stats",
    "href": "heatmaps.html#heatmap-in-package-stats",
    "title": "Heatmaps with Package Seriation",
    "section": "heatmap in package stats",
    "text": "heatmap in package stats\nThis is R’s standard heatmap function. seriate() can be used to supply the reordered dendrogram.\n\no &lt;- seriate(Wood, method = \"Heatmap\", seriation_method = \"OLO\")\nheatmap(Wood, Rowv = as.dendrogram(o[[1]]), Colv = as.dendrogram(o[[2]]))\n\n\n\n\n\n\n\n\nWe can also supply the rank order for any seriation method as weights and the dendrogram will be reordered as close as possible to the seriation order.\n\no &lt;- seriate(Wood, method = \"Heatmap\", seriation_method = \"Spectral\")\nheatmap(Wood, Rowv =  get_rank(o, 1), Colv =  get_rank(o, 2))"
  },
  {
    "objectID": "heatmaps.html#package-superheat",
    "href": "heatmaps.html#package-superheat",
    "title": "Heatmaps with Package Seriation",
    "section": "Package superheat",
    "text": "Package superheat\nDefault ordering using hierarchical clustering.\n\nif (!require(\"superheat\")) install.packages(\"superheat\")\n\nLoading required package: superheat\n\nlibrary(\"superheat\")\nsuperheat(Wood, \n          row.dendrogram = TRUE, col.dendrogram = TRUE)\n\n\n\n\n\n\n\n\nOrder with `seriation`. Currently, the dendrograms are not reordered.\n\no &lt;- seriate(Wood, method = \"Heatmap\", seriation_method = \"OLO_ward\")\nsuperheat(Wood, \n          order.row = as.vector(get_order(o, 1)),  order.col = as.vector(get_order(o, 2)),\n          row.dendrogram = FALSE, col.dendrogram = FALSE)\n\no &lt;- seriate(Wood, method = \"Heatmap\", seriation_method = \"Spectral\")\nsuperheat(Wood, \n          order.row = as.vector(get_order(o, 1)),  order.col = as.vector(get_order(o, 2)),\n          row.dendrogram = FALSE, col.dendrogram = FALSE)"
  },
  {
    "objectID": "heatmaps.html#package-heatmaply",
    "href": "heatmaps.html#package-heatmaply",
    "title": "Heatmaps with Package Seriation",
    "section": "Package heatmaply",
    "text": "Package heatmaply\nThe package creates interactive heatmaps. It already uses package seriation for parameter seriate and supports the methods OLO () and GW.\n\nif (!suppressMessages(require(\"heatmaply\"))) install.packages(\"heatmaply\")\n\nlibrary(\"heatmaply\")\nheatmaply(Wood, seriate = \"none\", main = \"HC\")\nheatmaply(Wood, seriate = \"OLO\", main = \"OLO\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny dendrogram-based seriation method from seriation can be supplied.\n\no &lt;- seriate(Wood, method = \"Heatmap\", seriation_method = \"OLO_ward\")\nheatmaply(Wood, Rowv = o[[1]], Colv = o[[2]], main = \"OLO (Ward)\")\no &lt;- seriate(Wood, method = \"Heatmap\", seriation_method = \"Spectral\")\nheatmaply(Wood, Rowv = get_rank(o, 1), Colv = get_rank(o, 2), main = \"Spectral\")"
  },
  {
    "objectID": "correlation_matrix.html",
    "href": "correlation_matrix.html",
    "title": "Correlation Matix Visualization with Package Seriation",
    "section": "",
    "text": "A correlation matrix is a square, symmetric matrix showing the pairwise correlation coefficients between two sets of variables. Reordering the variables and plotting the matrix can help to find hidden patterns among the variables. The package seriation implements a large number of reordering methods (see: the list with all implemented seriation methods). seriation also provides a set of functions to display reordered matrices:\n\npimage()\nggpimage()\n\nHow to cite the seriation package:\n\nHahsler M, Hornik K, Buchta C (2008). “Getting things in order: An introduction to the R package seriation.” Journal of Statistical Software, 25(3), 1-34. ISSN 1548-7660, doi:10.18637/jss.v025.i03 https://doi.org/10.18637/jss.v025.i03.\n\n\n\nAs an example, we use the mtcars dataset which contains data about fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models).\n\nif (!require(\"seriation\")) install.packages(\"seriation\")\n\nLoading required package: seriation\n\nlibrary(\"seriation\")\ndata(\"mtcars\")\n\nDT::datatable(mtcars)\n\n\n\n\n\n\nWe calcualte a correlation matrix.\n\nm &lt;- cor(mtcars)\nround(m, 2)\n\n       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\nmpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\ncyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\ndisp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\nhp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\ndrat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\nwt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\nqsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\nvs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\nam    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\ngear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\ncarb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n\n\nWe first visualize the matrix without reordering and then use the order method \"AOE\". AOE stands for angle of eigenvectors and was proposed for correlation matrices by Friendly (2002).\n\npimage(m)\npimage(m, order = \"AOE\")\n\n\n\n\n\n\n\n\n\n\n\nThe reordering clearly shows that there is tow groups of highly correlated variables and these two groups have a strong negative correlation with each other.\n\n\n\nHere are some options. Many packages represent high correlations as blue and low correlations as red. We can set the colors that way or used other colors.\n\npimage(m, order = \"AOE\", col = rev(bluered()), diag = FALSE, upper_tri = FALSE)\npimage(m, order = \"AOE\", col = colorRampPalette(c(\"red\", \"white\", \"darkgreen\"))(100))\n\n\n\n\n\n\n\n\n\n\n\nThe plots are also available in ggplot2 versions.\n\nlibrary(\"ggplot2\")\n\nred_blue &lt;- scale_fill_gradient2(\n    low = scales::muted(\"red\"),\n    mid = \"white\",\n    high = scales::muted(\"blue\"),\n    na.value = \"white\",\n    midpoint = 0)\n\nggpimage(m, order = \"AOE\", diag = FALSE, upper_tri = FALSE) + red_blue\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nggpimage(m, order = \"AOE\") + scale_fill_gradient2(low = \"red\", high = \"darkgreen\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can apply any seriation method for distances to create an order. First, we convert the correlation matrix into a distance matrix using \\(d_{ij} = \\sqrt{1 - m_{ij}}\\). Then we can use the distances for seriation and use the resulting order to rearrange the rows and columns of the correlation matrix.\n\nd &lt;- as.dist(sqrt(1 - m))\n\no &lt;- seriate(d, \"MDS\")\npimage(m , order = c(o, o), main = \"MDS\", col = rev(bluered()))\n\no &lt;- seriate(d, \"ARSA\")\npimage(m , order = c(o, o), main = \"ARSA\", col = rev(bluered()))\n\no &lt;- seriate(d, \"OLO\")\npimage(m , order = c(o, o), main = \"OLO\", col = rev(bluered()))\n\no &lt;- seriate(d, \"R2E\")\npimage(m , order = c(o, o), main = \"R2E\", col = rev(bluered()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeveral other packages can be used to visualize and explore correlation structure. Some of these packages support reordering with the seriation package.\n\n\nThe order argument in corrgram accepts methods from package seriation.\n\nif (!require(\"corrgram\")) install.packages(\"corrgram\")\n\nLoading required package: corrgram\n\nlibrary(\"corrgram\")\n\ncorrgram(m, order = \"OLO\")\ncorrgram(m, order = \"OLO\", lower.panel=panel.shade, upper.panel=panel.pie)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe function rearrange() in package corrr accepts some methods from seriation. Here is a complete example that uses method \"R2E\".\n\nif (!require(\"corrr\")) install.packages(\"corrr\")\n\nLoading required package: corrr\n\nlibrary(\"corrr\")\n\nx &lt;- datasets::mtcars |&gt;\n       correlate() |&gt;   \n       focus(-cyl, -vs, mirror = TRUE) |&gt;  # remove 'cyl' and 'vs'\n       rearrange(method = \"R2E\") |&gt;  \n       shave()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\nrplot(x)\n\n\n\n\n\n\n\n\n\n\n\nPackage corrplot offers many visualization methods. Orders from package seriation can be used by permuting the correlation matrix before it is passed to corrplot().\n\nif (!require(\"corrplot\")) install.packages(\"corrplot\")\n\nLoading required package: corrplot\n\n\ncorrplot 0.92 loaded\n\nlibrary(\"corrplot\")\n\nd &lt;- as.dist(sqrt(1 - m))\no &lt;- seriate(d, \"R2E\")\nm_R2E &lt;- permute(m, c(o,o))\n\ncorrplot(m_R2E , order = \"original\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichael Hahsler, Kurt Hornik and Christian Buchta, Getting Things in Order: An Introduction to the R Package seriation, Journal of Statistical Software, 25(3), 2008. DOI: 10.18637/jss.v025.i03\nFriendly, M. (2002): Corrgrams: Exploratory Displays for Correlation Matrices. , (4), 316–324. DOI: 10.1198/000313002533"
  },
  {
    "objectID": "correlation_matrix.html#prepare-the-data",
    "href": "correlation_matrix.html#prepare-the-data",
    "title": "Correlation Matix Visualization with Package Seriation",
    "section": "",
    "text": "As an example, we use the mtcars dataset which contains data about fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models).\n\nif (!require(\"seriation\")) install.packages(\"seriation\")\n\nLoading required package: seriation\n\nlibrary(\"seriation\")\ndata(\"mtcars\")\n\nDT::datatable(mtcars)\n\n\n\n\n\n\nWe calcualte a correlation matrix.\n\nm &lt;- cor(mtcars)\nround(m, 2)\n\n       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\nmpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\ncyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\ndisp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\nhp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\ndrat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\nwt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\nqsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\nvs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\nam    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\ngear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\ncarb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n\n\nWe first visualize the matrix without reordering and then use the order method \"AOE\". AOE stands for angle of eigenvectors and was proposed for correlation matrices by Friendly (2002).\n\npimage(m)\npimage(m, order = \"AOE\")\n\n\n\n\n\n\n\n\n\n\n\nThe reordering clearly shows that there is tow groups of highly correlated variables and these two groups have a strong negative correlation with each other."
  },
  {
    "objectID": "correlation_matrix.html#visualization-options",
    "href": "correlation_matrix.html#visualization-options",
    "title": "Correlation Matix Visualization with Package Seriation",
    "section": "",
    "text": "Here are some options. Many packages represent high correlations as blue and low correlations as red. We can set the colors that way or used other colors.\n\npimage(m, order = \"AOE\", col = rev(bluered()), diag = FALSE, upper_tri = FALSE)\npimage(m, order = \"AOE\", col = colorRampPalette(c(\"red\", \"white\", \"darkgreen\"))(100))\n\n\n\n\n\n\n\n\n\n\n\nThe plots are also available in ggplot2 versions.\n\nlibrary(\"ggplot2\")\n\nred_blue &lt;- scale_fill_gradient2(\n    low = scales::muted(\"red\"),\n    mid = \"white\",\n    high = scales::muted(\"blue\"),\n    na.value = \"white\",\n    midpoint = 0)\n\nggpimage(m, order = \"AOE\", diag = FALSE, upper_tri = FALSE) + red_blue\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nggpimage(m, order = \"AOE\") + scale_fill_gradient2(low = \"red\", high = \"darkgreen\")\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale."
  },
  {
    "objectID": "correlation_matrix.html#using-other-seriation-methods",
    "href": "correlation_matrix.html#using-other-seriation-methods",
    "title": "Correlation Matix Visualization with Package Seriation",
    "section": "",
    "text": "We can apply any seriation method for distances to create an order. First, we convert the correlation matrix into a distance matrix using \\(d_{ij} = \\sqrt{1 - m_{ij}}\\). Then we can use the distances for seriation and use the resulting order to rearrange the rows and columns of the correlation matrix.\n\nd &lt;- as.dist(sqrt(1 - m))\n\no &lt;- seriate(d, \"MDS\")\npimage(m , order = c(o, o), main = \"MDS\", col = rev(bluered()))\n\no &lt;- seriate(d, \"ARSA\")\npimage(m , order = c(o, o), main = \"ARSA\", col = rev(bluered()))\n\no &lt;- seriate(d, \"OLO\")\npimage(m , order = c(o, o), main = \"OLO\", col = rev(bluered()))\n\no &lt;- seriate(d, \"R2E\")\npimage(m , order = c(o, o), main = \"R2E\", col = rev(bluered()))"
  },
  {
    "objectID": "correlation_matrix.html#other-packages",
    "href": "correlation_matrix.html#other-packages",
    "title": "Correlation Matix Visualization with Package Seriation",
    "section": "",
    "text": "Several other packages can be used to visualize and explore correlation structure. Some of these packages support reordering with the seriation package.\n\n\nThe order argument in corrgram accepts methods from package seriation.\n\nif (!require(\"corrgram\")) install.packages(\"corrgram\")\n\nLoading required package: corrgram\n\nlibrary(\"corrgram\")\n\ncorrgram(m, order = \"OLO\")\ncorrgram(m, order = \"OLO\", lower.panel=panel.shade, upper.panel=panel.pie)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe function rearrange() in package corrr accepts some methods from seriation. Here is a complete example that uses method \"R2E\".\n\nif (!require(\"corrr\")) install.packages(\"corrr\")\n\nLoading required package: corrr\n\nlibrary(\"corrr\")\n\nx &lt;- datasets::mtcars |&gt;\n       correlate() |&gt;   \n       focus(-cyl, -vs, mirror = TRUE) |&gt;  # remove 'cyl' and 'vs'\n       rearrange(method = \"R2E\") |&gt;  \n       shave()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\nrplot(x)\n\n\n\n\n\n\n\n\n\n\n\nPackage corrplot offers many visualization methods. Orders from package seriation can be used by permuting the correlation matrix before it is passed to corrplot().\n\nif (!require(\"corrplot\")) install.packages(\"corrplot\")\n\nLoading required package: corrplot\n\n\ncorrplot 0.92 loaded\n\nlibrary(\"corrplot\")\n\nd &lt;- as.dist(sqrt(1 - m))\no &lt;- seriate(d, \"R2E\")\nm_R2E &lt;- permute(m, c(o,o))\n\ncorrplot(m_R2E , order = \"original\")"
  },
  {
    "objectID": "correlation_matrix.html#references",
    "href": "correlation_matrix.html#references",
    "title": "Correlation Matix Visualization with Package Seriation",
    "section": "",
    "text": "Michael Hahsler, Kurt Hornik and Christian Buchta, Getting Things in Order: An Introduction to the R Package seriation, Journal of Statistical Software, 25(3), 2008. DOI: 10.18637/jss.v025.i03\nFriendly, M. (2002): Corrgrams: Exploratory Displays for Correlation Matrices. , (4), 316–324. DOI: 10.1198/000313002533"
  },
  {
    "objectID": "seriation_cluster_evaluation.html",
    "href": "seriation_cluster_evaluation.html",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "",
    "text": "Dissimilarity plots (Hahsler and Hornik, 2011) can be used to visually inspect the quality of a clustering solution. The plot uses a image plots of the reordered dissimilarity matrix organized by the clusters to display the clustered data. This display allows the user to visually assess clustering quality. Dissimilarity plots are implemented in the R package seriation.\nThe following examples show how to use dissimilarity plots. The examples are an updated version of the examples in the paper Hahsler and Hornik (2011) using tidyverse, ggplot2, and the latest version of the seriation package."
  },
  {
    "objectID": "seriation_cluster_evaluation.html#introduction",
    "href": "seriation_cluster_evaluation.html#introduction",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "",
    "text": "Dissimilarity plots (Hahsler and Hornik, 2011) can be used to visually inspect the quality of a clustering solution. The plot uses a image plots of the reordered dissimilarity matrix organized by the clusters to display the clustered data. This display allows the user to visually assess clustering quality. Dissimilarity plots are implemented in the R package seriation.\nThe following examples show how to use dissimilarity plots. The examples are an updated version of the examples in the paper Hahsler and Hornik (2011) using tidyverse, ggplot2, and the latest version of the seriation package."
  },
  {
    "objectID": "seriation_cluster_evaluation.html#examples",
    "href": "seriation_cluster_evaluation.html#examples",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "Examples",
    "text": "Examples\nWe load the used packages and set a seed for the random number generator to make the examples reproducible.\n\nlibrary(\"seriation\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\n\nset.seed(1234)\n\n\nRuspini: A simple dataset\nThe Ruspini dataset from package cluster is a popular dataset for illustrating clustering techniques. It consists of 75 points in two-dimensional space with four clearly distinguishable groups and thus is easy to cluster.\nThe data points (rows) are called objects. We randomize the order of the objects to avoid the order of the data affecting the methods.\n\nlibrary(cluster)\ndata(ruspini)\nruspini &lt;- ruspini |&gt; sample_frac()\nhead(ruspini)\n\n    x   y\n28 38 143\n22 32 149\n9  18  61\n5  13  49\n38 53 144\n16 28  60\n\nplot(ruspini)\n\n\n\n\n\n\n\n\nThe scatter plot shows that the data points fall into four well defined groups.\nWe cluster with k-means to find 4 groups and a produce dissimilarity plot. For comparison, we also add a cluster plot (clusplot() from package cluster). For the dissimilarity plot, we first calculate a distance matrix. We use the ggplot2 version of the dissimilarity plot (a non-ggplot version is available using function dissplot() with the same parameters).\n\ncl_ruspini &lt;- kmeans(ruspini, centers = 4, nstart = 5)\n\nd_ruspini &lt;- ruspini |&gt; dist()\nggdissplot(d_ruspini, cl_ruspini$cluster) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(ruspini, cl_ruspini$cluster, labels = 4)\n\n\n\n\n\n\n\n\n\n\n\nThe cluster plot (to the right) shows that the clusters were easy to find.\nDissimilarity plots visualize the distances between points in a distance matrix. A distance matrix for \\(n\\) objects is a \\(n \\times n\\) matrix with pairwise distances as values. The diagonal contains the distances between each object and itself and therefore is always zero. In this visualization, low distance values are shown using a darker color. The result of a “good” clustering should be a matrix with low dissimilarity values forming blocks around the main diagonal corresponding to the clusters.\nThe dissimilarity plot shows a good clustering structure with the clusters forming four dark squares. The lower triangle shows the pairwise distances and the upper triangle shows cluster averages. The clusters are ordered by similarity indicating that cluster 1 and 3 are similar and clusters 1 and 2 are the most dissimilar.\nDeciding on the number of clusters is a difficult problem. Next, we look at what happens when we misspecify the number of clusters. First, we use too few clusters with \\(k = 3\\).\n\ncl_ruspini3 &lt;- kmeans(ruspini, center=3, nstart = 5)\n\nggdissplot(d_ruspini, cl_ruspini3$cluster) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(ruspini, cl_ruspini3$cluster, labels = 4)\n\n\n\n\n\n\n\n\n\n\n\nIn the cluster plot to the right, we see that the cluster plot merges two clusters together.\nThe dissimilarity plot shows that cluster 2 actually consists of two clusters. This can be seen by the two triangles separated by a lighter square inside the cluster.\nNext, we use too many clusters with \\(k=7\\).\n\ncl_ruspini7 &lt;- kmeans(ruspini, centers=7)\n\nggdissplot(d_ruspini, cl_ruspini7$cluster) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(ruspini, cl_ruspini7$cluster)\n\n\n\n\n\n\n\n\n\n\n\nThe cluster plot to the right shows that 7 may be a reasonable number to identify smaller groups. However, the dissimilarity plot also rearranges the clusters and still clearly shows that there are actually four well-defined clusters in the data. Dissimilarity plots make detecting a misspecification of \\(k\\) a lot easier.\nWe can also use dissimilarity plots for exploring data without clustering.\n\nggdissplot(d_ruspini) + ggtitle(\"Dissimilarity Plot\")\n\n\n\n\n\n\n\n\n\n\nVotes: High dimensional data\nThe Congressional Voting Records data set is available via the UCI Repository of Machine Learning Databases. This data set includes votes (voted for, voted against, not voted) for each of the 435 congressmen of the U.S. House of Representatives on the 16 key votes during the second session of 1984.\nTo preserve the information that some congressmen, possibly on purpose, did not vote on some topics, we encoded the 16 votes using 32 binary variables, two for each key vote. A 1 for the first variable codes for a vote in favor, a 1 for the second variable codes for a vote against, and a 0 for both indicates that the congressman did not vote on the topic. Then we used the Jaccard index (called “binary” in dist()) to calculate a dissimilarity matrix between congressmen.\n\nlibrary(cluster)\n\ndata(Votes, package = \"cba\")\nx &lt;- cba::as.dummy(Votes[-17])\nd_votes &lt;- dist(x, method = \"binary\")\n\nSince our distances are not Euclidean now, we use PAM for clustering. PAM is similar to k-means, but accepts as input a distance matrix. We cluster with \\(k=2\\), since we suspect two main political groups in the data.\n\nlabels_votes2 &lt;- pam(d_votes, k=2, cluster.only = TRUE)\n\nggdissplot(d_votes, labels_votes2) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(d_votes, diss = TRUE, labels_votes2, labels = 4)\n\n\n\n\n\n\n\n\n\n\n\nThis clearly gives a good clustering result. The dissimilarity plot shows that the two groups are slightly different in size (number of representatives) and that the members of the smaller group vote slightly more similar with each other (a slightly darker color). We also see that the clusters are not completely clear inside with lighter streaks on one side.\nMaybe there are several sub-groups in each political party. We cluster again with \\(k = 12\\).\n\nlabels_votes12 &lt;- pam(d_votes, k=12, cluster.only = TRUE)\n\nggdissplot(d_votes, labels_votes12) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(d_votes, diss = TRUE, labels_votes12, labels = 4)\n\n\n\n\n\n\n\n\n\n\n\nThe cluster plot gets too complicated, but the dissimilarity plot shows that one group has a very compact core (a dark square) with a few more clusters more loosely connected. The other group is slightly larger, but consists of many more diverse clusters. There are two clusters (9 and 3) with similarity to both groups. Analyzing the party affiliation shows that the more compact group consists of representatives of the republican party, while the more diverse group contains democrats."
  },
  {
    "objectID": "seriation_cluster_evaluation.html#conclusion",
    "href": "seriation_cluster_evaluation.html#conclusion",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "Conclusion",
    "text": "Conclusion\nDissimilarity plots scale well with the dimensionality of the data and by reordering clusters and objects within clusters can provide a very concise structural representation of the clustering. Dissimilarity plots are also helpful in spotting the mis-specification of the number of clusters used for partitioning.\nDissimilarity plots are implemented in the package seriation and are easy to use. Details on the method can be found in Hahsler and Hornik (2011)."
  },
  {
    "objectID": "seriation_cluster_evaluation.html#references",
    "href": "seriation_cluster_evaluation.html#references",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "References",
    "text": "References\n\nHahsler, M. and Hornik, K. (2011): Dissimilarity plots: A visual exploration tool for partitional clustering. Journal of Computational and Graphical Statistics, 10(2):335–354. doi:10.1198/jcgs.2010.09139 (read the preprint)"
  },
  {
    "objectID": "comparison.html",
    "href": "comparison.html",
    "title": "A Comparison of Seriation Methods",
    "section": "",
    "text": "This document compares the seriation methods available in package seriation using the popular Iris data set and randomize the order of the objects.\n\nlibrary(\"seriation\")\n\ndata(\"iris\")\nx &lt;- as.matrix(iris[sample(nrow(iris)), -5])\nd &lt;- dist(x)"
  },
  {
    "objectID": "comparison.html#comparison-between-methods",
    "href": "comparison.html#comparison-between-methods",
    "title": "A Comparison of Seriation Methods",
    "section": "Comparison between methods",
    "text": "Comparison between methods\nWe can compare the seriation methods by how similar the orders are that they produce (measured using Spearman). The following code calculates distances between orders and then performs hierarchical clustering.\n\ndst &lt;- ser_dist(orders) \nhc &lt;- permute(hclust(dst), order = \"OLO\", dist = dst)\nplot(hc)\n\n\n\n\n\n\n\n\nThe reordered dendrogram clearly shows a group of methods based on hierarchical clustering focused on path length and another group that tries to optimize the other seriation measures.\nHere is a table to compare the seriation methods on different criterion measures. Use the interactive table to sort the methods given different measures. Note that some are maximized and some should be minimized. Details about the measures can be found in the manual page for criterion().\n\nlibrary(DT)\ndatatable(round(criterion, 2), extensions = \"FixedColumns\",\n    options = list(paging = TRUE, searching = TRUE, info = FALSE,\n      sort = TRUE, scrollX = TRUE, fixedColumns = list(leftColumns = 1))) %&gt;%\n    formatRound(columns = colnames(criterion) , mark = \"\", digits=1)"
  },
  {
    "objectID": "comparison.html#visualize-the-results",
    "href": "comparison.html#visualize-the-results",
    "title": "A Comparison of Seriation Methods",
    "section": "Visualize the results",
    "text": "Visualize the results\nPlot the reordered dissimilarity matrices. Dark blocks along the main diagonal mean that the order reveals a “cluster” of similar objects. The Iris dataset contains three species, but two of them are very similar, so we expect to see one smaller block and one larger block.\n\nfor (n in names(orders))\n  pimage(d, orders[[n]], main = n , key = FALSE)"
  },
  {
    "objectID": "comparison.html#comparison-between-methods-1",
    "href": "comparison.html#comparison-between-methods-1",
    "title": "A Comparison of Seriation Methods",
    "section": "Comparison between methods",
    "text": "Comparison between methods\n\ndatatable(round(criterion, 2), extensions = \"FixedColumns\",\n    options = list(paging = TRUE, searching = TRUE, info = FALSE,\n      sort = TRUE, scrollX = TRUE, fixedColumns = list(leftColumns = 1))) %&gt;%\n    formatRound(columns = colnames(criterion) , mark = \"\", digits = 1)"
  },
  {
    "objectID": "comparison.html#visualize-the-results-1",
    "href": "comparison.html#visualize-the-results-1",
    "title": "A Comparison of Seriation Methods",
    "section": "Visualize the results",
    "text": "Visualize the results\n\nbest_to_worse &lt;- order(criterion[[\"Moore_stress\"]], decreasing = FALSE)\n\norders &lt;- orders[best_to_worse]\ncriterion &lt;- criterion[best_to_worse, ]\n\n\nfor (n in names(orders))\n  pimage(x, orders[[n]], main = n , key = FALSE)"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "",
    "text": "Dissimilarity plots (Hahsler and Hornik, 2011) can be used to visually inspect the quality of a clustering solution. The plot uses a image plots of the reordered dissimilarity matrix organized by the clusters to display the clustered data. This display allows the user to visually assess clustering quality. Dissimilarity plots are implemented in the R package seriation.\nThe following examples show how to use dissimilarity plots. The examples are an updated version of the examples in the paper Hahsler and Hornik (2011) using tidyverse, ggplot2, and the latest version of the seriation package."
  },
  {
    "objectID": "clustering.html#introduction",
    "href": "clustering.html#introduction",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "",
    "text": "Dissimilarity plots (Hahsler and Hornik, 2011) can be used to visually inspect the quality of a clustering solution. The plot uses a image plots of the reordered dissimilarity matrix organized by the clusters to display the clustered data. This display allows the user to visually assess clustering quality. Dissimilarity plots are implemented in the R package seriation.\nThe following examples show how to use dissimilarity plots. The examples are an updated version of the examples in the paper Hahsler and Hornik (2011) using tidyverse, ggplot2, and the latest version of the seriation package."
  },
  {
    "objectID": "clustering.html#examples",
    "href": "clustering.html#examples",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "Examples",
    "text": "Examples\nWe load the used packages and set a seed for the random number generator to make the examples reproducible.\n\nlibrary(\"seriation\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\n\nset.seed(1234)\n\n\nRuspini: A simple dataset\nThe Ruspini dataset from package cluster is a popular dataset for illustrating clustering techniques. It consists of 75 points in two-dimensional space with four clearly distinguishable groups and thus is easy to cluster.\nThe data points (rows) are called objects. We randomize the order of the objects to avoid the order of the data affecting the methods.\n\nlibrary(cluster)\ndata(ruspini)\nruspini &lt;- ruspini |&gt; sample_frac()\nhead(ruspini)\n\n    x   y\n28 38 143\n22 32 149\n9  18  61\n5  13  49\n38 53 144\n16 28  60\n\nplot(ruspini)\n\n\n\n\n\n\n\n\nThe scatter plot shows that the data points fall into four well defined groups.\nWe cluster with k-means to find 4 groups and a produce dissimilarity plot. For comparison, we also add a cluster plot (clusplot() from package cluster). For the dissimilarity plot, we first calculate a distance matrix. We use the ggplot2 version of the dissimilarity plot (a non-ggplot version is available using function dissplot() with the same parameters).\n\ncl_ruspini &lt;- kmeans(ruspini, centers = 4, nstart = 5)\n\nd_ruspini &lt;- ruspini |&gt; dist()\nggdissplot(d_ruspini, cl_ruspini$cluster) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(ruspini, cl_ruspini$cluster, labels = 4)\n\n\n\n\n\n\n\n\n\n\n\nThe cluster plot (to the right) shows that the clusters were easy to find.\nDissimilarity plots visualize the distances between points in a distance matrix. A distance matrix for \\(n\\) objects is a \\(n \\times n\\) matrix with pairwise distances as values. The diagonal contains the distances between each object and itself and therefore is always zero. In this visualization, low distance values are shown using a darker color. The result of a “good” clustering should be a matrix with low dissimilarity values forming blocks around the main diagonal corresponding to the clusters.\nThe dissimilarity plot shows a good clustering structure with the clusters forming four dark squares. The lower triangle shows the pairwise distances and the upper triangle shows cluster averages. The clusters are ordered by similarity indicating that cluster 1 and 3 are similar and clusters 1 and 2 are the most dissimilar.\nDeciding on the number of clusters is a difficult problem. Next, we look at what happens when we misspecify the number of clusters. First, we use too few clusters with \\(k = 3\\).\n\ncl_ruspini3 &lt;- kmeans(ruspini, center=3, nstart = 5)\n\nggdissplot(d_ruspini, cl_ruspini3$cluster) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(ruspini, cl_ruspini3$cluster, labels = 4)\n\n\n\n\n\n\n\n\n\n\n\nIn the cluster plot to the right, we see that the cluster plot merges two clusters together.\nThe dissimilarity plot shows that cluster 2 actually consists of two clusters. This can be seen by the two triangles separated by a lighter square inside the cluster.\nNext, we use too many clusters with \\(k=7\\).\n\ncl_ruspini7 &lt;- kmeans(ruspini, centers=7)\n\nggdissplot(d_ruspini, cl_ruspini7$cluster) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(ruspini, cl_ruspini7$cluster)\n\n\n\n\n\n\n\n\n\n\n\nThe cluster plot to the right shows that 7 may be a reasonable number to identify smaller groups. However, the dissimilarity plot also rearranges the clusters and still clearly shows that there are actually four well-defined clusters in the data. Dissimilarity plots make detecting a misspecification of \\(k\\) a lot easier.\nWe can also use dissimilarity plots for exploring data without clustering.\n\nggdissplot(d_ruspini) + ggtitle(\"Dissimilarity Plot\")\n\n\n\n\n\n\n\n\n\n\nVotes: High dimensional data\nThe Congressional Voting Records data set is available via the UCI Repository of Machine Learning Databases. This data set includes votes (voted for, voted against, not voted) for each of the 435 congressmen of the U.S. House of Representatives on the 16 key votes during the second session of 1984.\nTo preserve the information that some congressmen, possibly on purpose, did not vote on some topics, we encoded the 16 votes using 32 binary variables, two for each key vote. A 1 for the first variable codes for a vote in favor, a 1 for the second variable codes for a vote against, and a 0 for both indicates that the congressman did not vote on the topic. Then we used the Jaccard index (called “binary” in dist()) to calculate a dissimilarity matrix between congressmen.\n\nlibrary(cluster)\n\ndata(Votes, package = \"cba\")\nx &lt;- cba::as.dummy(Votes[-17])\nd_votes &lt;- dist(x, method = \"binary\")\n\nSince our distances are not Euclidean now, we use PAM for clustering. PAM is similar to k-means, but accepts as input a distance matrix. We cluster with \\(k=2\\), since we suspect two main political groups in the data.\n\nlabels_votes2 &lt;- pam(d_votes, k=2, cluster.only = TRUE)\n\nggdissplot(d_votes, labels_votes2) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(d_votes, diss = TRUE, labels_votes2, labels = 4)\n\n\n\n\n\n\n\n\n\n\n\nThis clearly gives a good clustering result. The dissimilarity plot shows that the two groups are slightly different in size (number of representatives) and that the members of the smaller group vote slightly more similar with each other (a slightly darker color). We also see that the clusters are not completely clear inside with lighter streaks on one side.\nMaybe there are several sub-groups in each political party. We cluster again with \\(k = 12\\).\n\nlabels_votes12 &lt;- pam(d_votes, k=12, cluster.only = TRUE)\n\nggdissplot(d_votes, labels_votes12) + ggtitle(\"Dissimilarity Plot\")\n\nclusplot(d_votes, diss = TRUE, labels_votes12, labels = 4)\n\n\n\n\n\n\n\n\n\n\n\nThe cluster plot gets too complicated, but the dissimilarity plot shows that one group has a very compact core (a dark square) with a few more clusters more loosely connected. The other group is slightly larger, but consists of many more diverse clusters. There are two clusters (9 and 3) with similarity to both groups. Analyzing the party affiliation shows that the more compact group consists of representatives of the republican party, while the more diverse group contains democrats."
  },
  {
    "objectID": "clustering.html#conclusion",
    "href": "clustering.html#conclusion",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "Conclusion",
    "text": "Conclusion\nDissimilarity plots scale well with the dimensionality of the data and by reordering clusters and objects within clusters can provide a very concise structural representation of the clustering. Dissimilarity plots are also helpful in spotting the mis-specification of the number of clusters used for partitioning.\nDissimilarity plots are implemented in the package seriation and are easy to use. Details on the method can be found in Hahsler and Hornik (2011)."
  },
  {
    "objectID": "clustering.html#references",
    "href": "clustering.html#references",
    "title": "How to Evaluate Clusters Using Dissimilarity Plots",
    "section": "References",
    "text": "References\n\nHahsler, M. and Hornik, K. (2011): Dissimilarity plots: A visual exploration tool for partitional clustering. Journal of Computational and Graphical Statistics, 10(2):335–354. doi:10.1198/jcgs.2010.09139 (read the preprint)"
  }
]